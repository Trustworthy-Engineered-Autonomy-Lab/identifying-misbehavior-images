{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52be854b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 06:34:33.374437: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-16 06:34:33.423177: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def clear_memory():\n",
    "    K.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe17a706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91994d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape, Conv2D, Conv2DTranspose, Lambda, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import re\n",
    "\n",
    "# Keeping the essential data loading function\n",
    "def load_catalogs(folder: str):\n",
    "    _img_name, _angle, _throttle, _image = [], [], [], []\n",
    "\n",
    "    for _file in sorted(glob.glob(f\"{folder}/*.catalog\"), key=lambda x: [int(c) if c.isdigit() else c for c in re.split(r'(\\d+)', x)]):\n",
    "        with open(_file) as f:\n",
    "            for _line in f:\n",
    "                _name = _line.split()[7][1:-2]\n",
    "                _img_path = os.path.join(f\"{folder}/images\", _name)\n",
    "                \n",
    "                if not os.path.isfile(_img_path):\n",
    "                    continue\n",
    "                \n",
    "                _img = cv2.imread(_img_path)\n",
    "                if _img is None:\n",
    "                    continue\n",
    "                \n",
    "                assert _img.shape == (224, 224, 3), f\"img {_name} has shape {_img.shape}\"\n",
    "                \n",
    "                _image.append(_img)\n",
    "                _angle.append(float(_line.split()[9][0:-1]))\n",
    "                _throttle.append(float(_line.split()[13][0:-1]))\n",
    "\n",
    "    print(f'Image count: {len(_image)}')\n",
    "    return np.array(_image), np.array(_angle), np.array(_throttle)\n",
    "\n",
    "# Improved data preprocessing\n",
    "def data_preprocessing(images):\n",
    "    return images.astype(\"float32\") / 255.0\n",
    "\n",
    "# Enhanced encoder with BatchNormalization\n",
    "def build_encoder(input_shape=(224, 224, 3), latent_dim=64):\n",
    "    inputs = Input(shape=input_shape, name=\"encoder_input\")\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    z_mean = Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Lambda(lambda args: args[0] + tf.exp(0.5 * args[1]) * tf.random.normal(tf.shape(args[0])))([z_mean, z_log_var])\n",
    "    \n",
    "    return Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "# Enhanced decoder with BatchNormalization\n",
    "def build_decoder(latent_dim=64):\n",
    "    latent_inputs = Input(shape=(latent_dim,), name=\"decoder_input\")\n",
    "    \n",
    "    x = Dense(28 * 28 * 128, activation=\"relu\")(latent_inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Reshape((28, 28, 128))(x)\n",
    "    \n",
    "    x = Conv2DTranspose(64, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv2DTranspose(32, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv2DTranspose(32, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    outputs = Conv2DTranspose(3, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "    \n",
    "    return Model(latent_inputs, outputs, name=\"decoder\")\n",
    "\n",
    "# Improved VAE with better loss balancing\n",
    "def build_vae(input_shape=(224, 224, 3), latent_dim=64, beta=0.01):\n",
    "    encoder = build_encoder(input_shape, latent_dim)\n",
    "    decoder = build_decoder(latent_dim)\n",
    "    inputs = Input(shape=input_shape, name=\"vae_input\")\n",
    "    z_mean, z_log_var, z = encoder(inputs)\n",
    "    outputs = decoder(z)\n",
    "    vae = Model(inputs, outputs, name=\"vae\")\n",
    "    # Fixed reconstruction loss calculation\n",
    "    reconstruction_loss = tf.reduce_mean(\n",
    "        tf.keras.losses.binary_crossentropy(\n",
    "            tf.keras.backend.flatten(inputs),\n",
    "            tf.keras.backend.flatten(outputs)\n",
    "        )\n",
    "    )\n",
    "    kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "    vae_loss = reconstruction_loss + beta * kl_loss\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "    return vae, encoder\n",
    "\n",
    "# Improved control model for both angle and throttle\n",
    "def build_control_model(input_dim):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    angle_output = Dense(1, activation='tanh', name='angle')(x)  # tanh for angle (-1 to 1)\n",
    "    throttle_output = Dense(1, activation='sigmoid', name='throttle')(x)  # sigmoid for throttle (0 to 1)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[angle_output, throttle_output])\n",
    "    model.compile(optimizer='adam',\n",
    "                 loss={'angle': 'mse', 'throttle': 'mse'},\n",
    "                 metrics={'angle': ['mae', 'mse'], 'throttle': ['mae', 'mse']})\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Main training and evaluation pipeline\n",
    "def train_and_evaluate(data_folder):\n",
    "    # Load and preprocess data\n",
    "    images, angles, throttles = load_catalogs(data_folder)\n",
    "    images = data_preprocessing(images)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train_angle, y_test_angle, y_train_throttle, y_test_throttle = train_test_split(\n",
    "        images, angles, throttles, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Build and train VAE\n",
    "    vae, encoder = build_vae()\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    vae.fit(X_train, X_train,\n",
    "            epochs=10,\n",
    "            batch_size=16,\n",
    "            validation_data=(X_test, X_test),\n",
    "            callbacks=[early_stopping, reduce_lr])\n",
    "    \n",
    "    # Extract latent representations\n",
    "    latent_train = encoder.predict(X_train)[2]\n",
    "    latent_test = encoder.predict(X_test)[2]\n",
    "    \n",
    "    # Train control model\n",
    "    control_model = build_control_model(latent_train.shape[1])\n",
    "    history = control_model.fit(latent_train,\n",
    "                              {'angle': y_train_angle, 'throttle': y_train_throttle},\n",
    "                              validation_data=(latent_test, {'angle': y_test_angle, 'throttle': y_test_throttle}),\n",
    "                              epochs=10,\n",
    "                              batch_size=16,\n",
    "                              callbacks=[early_stopping, reduce_lr])\n",
    "    \n",
    "    # Evaluate and visualize results\n",
    "    angle_pred, throttle_pred = control_model.predict(latent_test)\n",
    "    \n",
    "    # Calculate metrics for both controls\n",
    "    metrics = {\n",
    "        'angle': {\n",
    "            'mse': mean_squared_error(y_test_angle, angle_pred),\n",
    "            'r2': r2_score(y_test_angle, angle_pred)\n",
    "        },\n",
    "        'throttle': {\n",
    "            'mse': mean_squared_error(y_test_throttle, throttle_pred),\n",
    "            'r2': r2_score(y_test_throttle, throttle_pred)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(f\"Angle - MSE: {metrics['angle']['mse']:.4f}, R2: {metrics['angle']['r2']:.4f}\")\n",
    "    print(f\"Throttle - MSE: {metrics['throttle']['mse']:.4f}, R2: {metrics['throttle']['r2']:.4f}\")\n",
    "    \n",
    "    return vae, encoder, control_model, metrics, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e4546",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"/home/UFAD/mohitkukreja/Documents/data_perfect_driving\"\n",
    "    vae, encoder, control_model, metrics, history = train_and_evaluate(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e4c463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
